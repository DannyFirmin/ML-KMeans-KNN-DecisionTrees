{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management,<br/> Analysis and Security</h1>\n",
    "\n",
    "<h1 align='center'> Assignment - 2</h1>\n",
    "\n",
    "-----\n",
    "<br/>\n",
    "\n",
    "## Grading\n",
    "\n",
    "|**Maximum Marks**         |  **100**\n",
    "|--------------------------|--------\n",
    "|  **Weight**              |  **20% of the Total Course Grade**\n",
    "|  **Submission deadline** |  **7:00PM, Friday, May 24**\n",
    "|  **Submission mode**     |  **Electronic, Using GitLab <br/> One submission per group**\n",
    "|  **Estimated time**      |  **20 hours**\n",
    "|  **Penalty**             |  **100% after the deadline**\n",
    "  \n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "You need to submit the notebook `Assignment-2.ipynb` and any other additional files that you may have created / hyperlinked in this notebook, as part of your submission by pushing it to your forked GitLab repository. You need to add your group details below. Make sure your group works on and submits only have one fork of the assignment repository. \n",
    "\n",
    "\n",
    "### Note:\n",
    "\n",
    "* It is strongly advised to read the whole assignment before attempting it and have at least a cursory glance at the dataset in order to gauge the requirements and understand what you need to do as a bigger picture.\n",
    "\n",
    "* For answers requiring free form written text, use the designated cells denoted by `YOUR ANSWER HERE` -- double click on the cell to write inside them.\n",
    "\n",
    "* For all coding questions please write your code after the comment `YOUR CODE HERE`.\n",
    "\n",
    "* In the process of testing your code, you can insert more cells or use print statements for debugging, but when submitting your file remember to remove these cells and calls respectively.\n",
    "\n",
    "* You will be marked on **correctness** and **readability** of your code, if your marker can't understand your code your marks may be deducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Group Number :  2420_A2_Grp12\n",
    "\n",
    "### Student IDs: u6555407, u6588836, u6611178\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING FREQUENTLY USED PYTHON MODULES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import os\n",
    "plt.style.use('seaborn-notebook')\n",
    "%matplotlib inline\n",
    "\n",
    "# JUST TO MAKE SURE SOME WARNINGS ARE IGNORED \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT ANY OTHER REQUIRED MODULES IN THIS CELL\n",
    "### TODO SECTION A: q4 & SECTION B Q1.1\n",
    "#### We might need actor name and corresponding actor id.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import combinations \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A - Database Management (15 Marks)\n",
    "\n",
    "You have been given the following database containing 15 tables relating to **DVD Movie Rentals**. The data contained in these tables is as follows -\n",
    "\n",
    "|  **Table**               |  **Data Description**      |\n",
    "|--------------------------|----------------------------|\n",
    "|  actor                   |  actors data including first name and last name     |\n",
    "|  film                    |  films data such as title, release year, length, rating, etc      |\n",
    "|  film_actor              |  stores the relationships between films and actors |\n",
    "|  category                |  stores film’s categories data |\n",
    "|  film_category           |  stores the relationships between films and categories | \n",
    "|  store                   |  store data including manager staff and address |\n",
    "|  inventory               |  stores inventory data |\n",
    "|  rental                  |  stores rental data |\n",
    "|  payment                 |  stores customer's payments |\n",
    "|  staff                   |  stores staff data |\n",
    "|  customer                |  stores customer data |\n",
    "|  address                 |  address data for staff and customers |\n",
    "|  city                    |  stores city data |\n",
    "|  country                 |  stores country data |\n",
    "\n",
    "Visualizing the relations between these tables can be aided by looking at the below **E-R Diagram**.\n",
    "\n",
    "<img src='./dvd_rental_er.png'>\n",
    "\n",
    "\n",
    "Based on your understanding of the relationships between these tables, answer the following questions by writing SQL queries to get the required data rows from the database and display them as a **Pandas dataframe**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### **NOTE**:\n",
    "For the following questions, in the CSIT labs you will be able to connect directly to the database using the below code within the notebook. From outside of the CSIT labs, you will need to perform SQL queries in your terminal by using [partch](https://cs.anu.edu.au/docs/student-computing-environment/linuxlabs/remoteaccess/#connectingtopartch3). Once you have the correct query, you may just fill in the boxes below.\n",
    "\n",
    "**Partch Instructions**\n",
    "1. Connect to partch as per the above hyperlink\n",
    "2. In your terminal, enter `psql` to access the sql database\n",
    "3. Enter `\\c dvdrental` to ensure you are accessing the assignment database (or simply specify dvdrental when connecting to the database server -- `psql dvdrental`).\n",
    "4. (Sanity Check) Enter `SELECT * FROM actor;` . If you receive the first row to be \"Penelope Guiness\", you should be good to go !\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect using psycopg2\n",
    "conn = psycopg2.connect(host=\"/var/run/postgresql\", database=\"dvdrental\")\n",
    "\n",
    "# Activate connection cursor\n",
    "curr = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(query):\n",
    "    # Select table and display\n",
    "    curr.execute(query)\n",
    "\n",
    "    # Fetches all the rows from the result of the query\n",
    "    rows = curr.fetchall()\n",
    "    \n",
    "    # Gets the column names for the table\n",
    "    colnames = [desc[0] for desc in curr.description]\n",
    "\n",
    "    # Converts into readable pandas dataframe\n",
    "    df_result = pd.DataFrame(rows, columns=colnames)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write an SQL query to determine the total sales from the rentals of the film 'Affair Prejudice' from the rental store with `store_id = 2`. Your query should result in a single column called `Total Rental Cost` with the value of the total cost of all these rentals.\n",
    "<span style= 'float: right;'><b>[3 marks]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = str(\"SELECT SUM(p.amount) AS TOTAL_RENTAL_COST\"\n",
    "            + \" FROM payment AS p,rental AS r,inventory AS i,film AS f\"\n",
    "            + \" WHERE p.rental_id=r.rental_id\"\n",
    "            + \" AND r.inventory_id=i.inventory_id\"\n",
    "            + \" AND i.film_id=f.film_id\"\n",
    "            + \" AND f.title LIKE 'Affair Prejudice'\"\n",
    "            + \" AND i.store_id=2;\")\n",
    "\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write an SQL query to retrieve the names of all customers who rented the film ‘Affair Prejudice’ from the store with `store_id = 1`. The result of your query should display each customer's first name and last name. \n",
    "<span style= 'float: right;'><b>[3 marks]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = str(\"SELECT c.first_name,c.last_name\"\n",
    "            + \" FROM customer AS c,rental AS r,inventory AS i,film AS f\"\n",
    "            + \" WHERE c.customer_id=r.customer_id\"\n",
    "            + \" AND r.inventory_id=i.inventory_id\"\n",
    "            + \" AND i.film_id=f.film_id\"\n",
    "            + \" AND f.title LIKE '%Affair Prejudice%'\"\n",
    "            + \" AND c.store_id=1;\")\n",
    "\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write an SQL query to retrieve the names of all those customers who have a total of more than 100 dollars in recorded payments. Your query's result should include each customer's first name, last name and customer ID. \n",
    "<span style= 'float: right;'><b>[4 marks]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = str(\"SELECT c.first_name,c.last_name,c.customer_id\"\n",
    "            + \" FROM customer AS c, payment AS p\"\n",
    "            + \" WHERE c.customer_id=P.customer_id\"\n",
    "            + \" GROUP BY c.customer_id HAVING SUM(p.amount)>100;\")\n",
    "\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write an SQL query to retrieve the names of all customers who have rented the movie ‘Angels Life’ from *both* stores with `store_id = 1` and `store_id = 2`. Your query's result should include each customer's first name, last name and customer ID.\n",
    "<span style= 'float: right;'><b>[5 marks]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = str(\"SELECT DISTINCT c.customer_id,c.first_name,c.last_name\"\n",
    "            + \" FROM store as s,customer as c, rental as r,inventory as i, film as f\"\n",
    "            + \" WHERE s.store_id=c.store_id\"\n",
    "            + \" AND c.customer_id=r.customer_id\"\n",
    "            + \" AND r.inventory_id=i.inventory_id\"\n",
    "            + \" AND i.film_id=f.film_id\"\n",
    "            + \" AND s.store_id IN (1,2)\"\n",
    "            + \" AND f.title='Angels Life'\"\n",
    "            + \" GROUP BY c.customer_id\"\n",
    "            + \" HAVING COUNT(c.customer_id)>=2;\")\n",
    "\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B - Data Acquisition (20 Marks)\n",
    "\n",
    "In this section, you need to read and import the data from the database into Python and make it ready to be used by your machine learning algorithms in the next section. **Your task here is to load the required columns from the various appropriate tables in the database to fuel the data required to train your machine learning models in the next section.** You can load the required data into one or more Pandas dataframes, to suit the the needs of different Machine Learning models in Section C. \n",
    "\n",
    "If you fail to do so, or an error in the previous section is preventing you from doing so, we can provide you with a **CSV of the required data**. If you choose to use this CSV, you will not receive any marks for this section, but you'll be able to do the following section without having to spend time on this section.  \n",
    "\n",
    "<span style='color:red;'><b>Note:</b> While you are provided the .csv files to use for development of the later questions at home, you must submit a copy of the code that can read the database to receive marks for this section.</span>\n",
    "<span style= 'float: right;'><b>[20 marks]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customer_rental = str(\"SELECT * FROM customer c\"\n",
    "                      + \" JOIN payment p\"\n",
    "                      + \" ON c.customer_id=p.customer_id\"\n",
    "                      + \" RIGHT JOIN rental r\"\n",
    "                      + \" ON p.rental_id=r.rental_id\"\n",
    "                      + \" JOIN inventory i\"\n",
    "                      + \" ON i.inventory_id=r.inventory_id\"\n",
    "                      + \" JOIN film f\"\n",
    "                      + \" ON i.film_id=f.film_id\"\n",
    "                      + \" JOIN film_category fc\"\n",
    "                      + \" ON fc.film_id=f.film_id\"\n",
    "                      + \" JOIN category\"\n",
    "                      + \" ON category.category_id=fc.category_id;\"\n",
    "                      )\n",
    "\n",
    "customer_country = str(\"SELECT * FROM customer c\"\n",
    "                       + \" JOIN address a\"\n",
    "                       + \" ON c.address_id=a.address_id\"\n",
    "                       + \" JOIN city\"\n",
    "                       + \" ON city.city_id=a.city_id\"\n",
    "                       + \" JOIN country\"\n",
    "                       + \" ON country.country_id=city.country_id;\")\n",
    "\n",
    "film_actor = str(\"SELECT * FROM film f\"\n",
    "                 + \" JOIN film_actor fa\"\n",
    "                 + \" ON fa.film_id=f.film_id\"\n",
    "                 + \" JOIN actor a\"\n",
    "                 + \" ON a.actor_id=fa.actor_id;\"\n",
    "                 )\n",
    "\n",
    "\n",
    "category_name=str(\"SELECT * FROM category;\")\n",
    "\n",
    "inventory=str(\"SELECT * FROM inventory;\")\n",
    "\n",
    "df_customer_rental = execute_sql(customer_rental)\n",
    "df_customer_country = execute_sql(customer_country)\n",
    "df_film_actor = execute_sql(film_actor)\n",
    "df_category_name=execute_sql(category_name)\n",
    "df_inventory=execute_sql(inventory)\n",
    "\n",
    "df_film_actor.to_csv(\"film_actor.csv\")\n",
    "df_customer_rental.to_csv(\"customer_rental.csv\")\n",
    "df_customer_country.to_csv(\"customer_country.csv\")\n",
    "df_inventory.to_csv(\"inventory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number copies for each film in the stores.\n",
    "df_inventory_count=df_inventory.groupby(\"film_id\").count().rename({\"store_id\":\"inventory_count\"},axis=1)\n",
    "df_inventory_count=df_inventory_count[\"inventory_count\"].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_country = df_customer_country.loc[:, ~df_customer_country.columns.duplicated()]\n",
    "df_customer_country = df_customer_country[[\"customer_id\", \"address_id\", \"active\", \"city_id\", \"country_id\", \"country\"]]\n",
    "\n",
    "df_customer_rental = df_customer_rental.loc[:, ~df_customer_rental.columns.duplicated()]\n",
    "df_customer_rental = df_customer_rental[\n",
    "    [\"customer_id\", \"store_id\", \"payment_id\", \"rental_id\", \"amount\", \"inventory_id\", \"film_id\", \"rental_duration\",\n",
    "     \"rental_rate\", \"length\", \"replacement_cost\", \"rating\", \"special_features\", \"fulltext\", \"category_id\"]]\n",
    "\n",
    "df_film_actor = df_film_actor.loc[:, ~df_film_actor.columns.duplicated()]\n",
    "df_film_actor[\"actor_name\"] = df_film_actor.first_name.map(str) + \" \" + df_film_actor.last_name\n",
    "df_film_actor = df_film_actor[[\"film_id\", \"actor_id\", \"actor_name\"]]\n",
    "df_film_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_rental_country = df_customer_rental.merge(df_customer_country, on='customer_id', how='inner')\n",
    "df_customer_rental_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Popularity \n",
    "df_popularity=df_film_actor.groupby(\"actor_id\").count()[\"actor_name\"]\n",
    "df_popularity=df_popularity.to_frame().rename({\"actor_name\":\"popularity\"},axis=1)\n",
    "df_film_actor=df_film_actor.merge(df_popularity,on=\"actor_id\")\n",
    "df_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating popularity_mean & popularity_max\n",
    "popularity_mean_list = []\n",
    "popularity_max_list = []\n",
    "filmID_list = []\n",
    "popularity_sum=[]\n",
    "actor_count=[]\n",
    "\n",
    "\n",
    "for ele in df_film_actor.groupby(df_film_actor['film_id']):\n",
    "    filmID_list.append(ele[1]['film_id'].values[0])\n",
    "    popularity_mean_list.append(np.mean(np.asarray(list(ele[1]['popularity']))))\n",
    "    popularity_max_list.append(max(list(ele[1]['popularity'])))\n",
    "    popularity_sum.append(sum(list(ele[1]['popularity'])))\n",
    "    actor_count.append(len(list(ele[1]['popularity'])))\n",
    "  \n",
    "    \n",
    "film_popularity= {'film_id': filmID_list,\"popularity_sum\":popularity_sum,\"actor_count\":actor_count,\"popularity_max\":popularity_max_list, 'popularity_mean': popularity_mean_list}\n",
    "\n",
    "df_film_popularity = pd.DataFrame(film_popularity)\n",
    "df_film_popularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_customer_rental.merge(df_film_popularity, on='film_id', how='inner')\n",
    "df_cleaned=df_cleaned.merge(df_inventory_count,on=\"film_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "bts = []\n",
    "tril = []\n",
    "comm = []\n",
    "for row in df_cleaned.special_features:  \n",
    "    if \"Deleted Scenes\" in row:\n",
    "        ds.append(1)\n",
    "    if \"Behind the Scenes\" in row:\n",
    "        bts.append(1)\n",
    "    if \"Trailers\" in row:\n",
    "        tril.append(1)\n",
    "    if \"Commentaries\" in row:\n",
    "        comm.append(1)\n",
    "    if \"Deleted Scenes\" not in row:\n",
    "        ds.append(0)\n",
    "    if \"Behind the Scenes\" not in row:\n",
    "        bts.append(0)\n",
    "    if \"Trailers\" not in row:\n",
    "        tril.append(0)\n",
    "    if \"Commentaries\" not in row:\n",
    "        comm.append(0)\n",
    "special_features = {'Deleted_Scenes': ds, 'Behind_the_Scenes': bts, 'Trailers':tril, 'Commentaries':comm}\n",
    "df_special_features = pd.DataFrame(data=special_features)\n",
    "df_cleaned = df_cleaned.join(df_special_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned=df_cleaned.merge(df_film_popularity, on='film_id', how='inner')\n",
    "# to be removed later on \n",
    "df_cleaned.to_csv('df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C - Machine Learning (55 Marks)\n",
    "<span style='color:purple;'><b>Note:</b> We have provided the .csv outputs for the files to use in these questions if you are working on the assignment at home. Please note if any of these questions refer to those csvs and not the data from Section B in your submitted assignment, you will receive no marks for Section B.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading & Minor Cleanup Of DataFrame\n",
    "df_cleaned = pd.read_csv(\"df_cleaned.csv\")\n",
    "df_cleaned = df_cleaned.drop([\"Unnamed: 0\"], axis=1)\n",
    "df_category_name=pd.read_csv(\"data/dvd_rental/category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_combinations(dataframe, dependent_variable):\n",
    "    '''Helper function for producing all possible combinations of independent variables \n",
    "       based on the given dataframe & the dependent_variable\n",
    "       @param:dataframe - The primary DataFrame in which your data resides.\n",
    "       @return:output - A list of list which has all possible combinations.'''\n",
    "    output = []\n",
    "    independent_variables = []\n",
    "    for var in dataframe.columns:\n",
    "        if var != dependent_variable:\n",
    "            independent_variables.append(var)\n",
    "    for i in range(len(independent_variables)):\n",
    "        for combination in list(combinations(independent_variables, i + 1)):\n",
    "            output.append(combination)\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_column(s,column_name=\"\",color=\"yellow\"):\n",
    "    '''\n",
    "    Highlights a column of a Dataframe\n",
    "    @param column_name : The column's name of the column which needs to be highlighted.\n",
    "    @return A list of CSS attributes.\n",
    "    '''\n",
    "    if s.name==column_name:\n",
    "        return ['background-color:'+color]*len(s)\n",
    "    else:\n",
    "        return ['']*len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Clustering (25 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 - Clustering Short Answers (8 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Can a Decision Tree be used to perform clustering? If so, explain how. If not, provide a counterexample showing how it is not suitable for the function.\n",
    "<span style= 'float: right;'><b>[4 marks]</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a given dataset,decision trees can be used to perform classification,not clustering [(Both are similar,but different)](https://techdifferences.com/difference-between-classification-and-clustering.html) .Decision Trees are a form of supervised learning which uses Heuristics(attribute selection measures) such as Information Gain,Gain Ratio or Gini index to decide the decision nodes of the tree.Decision trees require labeled data to train on in order to predict on an unlabeled dataset, and  thus by training on training dataset each node's conditions are set.Upon using a new data on this model to predict the class/label of the input it will refer to the appropriate label which is mapped to the Leaf node.However, if one tries to use clustering, the decision tree algorithm will not support it as it needs the labels in order to  train it's model.\n",
    "- Counter - Example:\n",
    "<img src='Decision Tree Paint.png' width=\"483\" height=\"300\">\n",
    "- Here the decision tree is trained on the training data.It isnt viable to train the decision tree model without any labels as it will make decision trees a unsupervised learning algorithm.Although there are unsupervised learning form of decision trees,[Resource](https://pdfs.semanticscholar.org/8996/148e8f0b34308e2d22f78ff89bf1f038d1d6.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Will a K-means Clustering Algorithm generate the same results each time? Provide examples on how this may or may not be the case.\n",
    "<span style= 'float: right;'><b>[4 marks]</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"> Your answer here: </span>\n",
    "\n",
    "(Assumed that the centroids are randomly assigned and the number of the centroids are the same each time)\n",
    "\n",
    "The results are usually different. But, it depends on the data and the centroids number. For example, if the data has obvious clusters, and the number of centroids are equal to its clusters. K-means Clustering Algorithm will generate the same result each time no matter where the initial centroids are. For other kinds of data, the result usually depends on the initial position of the centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 - K-Means Clustering Implementation (17 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering helps visualise a dataset based on attributes considered important to the data scientist and/or reader.  From the dataset acquired after completing **Section - B**, implement a `K-Means clustering algorithm` to help you cluster the dataset of customers on the basis of the movies they have rented from the DVD rental stores. Various attributes related to the movies such `rating`, `year`, `rental_rate` and `year` may be useful for this exercise. Another interesting attribute that you can look at is the `fulltext` of the movie. In terms of the customer details, you can look at which movies a customer has rented, and the total number of movies rented by a customer. After you've prepared your learning model, plot a **visualization** showing the different clusters. If you have used more than 2 features for your clustering, you are still expected to provide a visualization by reducing the dimensions into a 2D graph.\n",
    "\n",
    "It's upto you to decide how many clusters you would like to incorporate in your model. You are expected to justify all aspects of your implementation including the reasoning behind the choice of **the number of clusters** and **number of iterations** in your model. \n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution.</span>\n",
    "<span style= 'float: right;'><b>[15 marks total: 10 marks model, 5 marks justification & commentary]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Category data \"Rating\" from String to Numeric\n",
    "df_elbow=df_cleaned\n",
    "df_elbow['rating'] = df_elbow['rating'].astype(\"category\").cat.codes\n",
    "#    \tactor_count\n",
    "# Principal Component Analysis - Dimensionality Reduction\n",
    "pca = PCA(n_components=2)\n",
    "df_X = df_elbow[['rental_duration', 'rental_rate', 'length', 'replacement_cost', \"popularity_sum\",'rating', 'category_id']]\n",
    "df_X = df_X.dropna()\n",
    "pca.fit(df_X)\n",
    "transform = pca.transform(df_X)\n",
    "\n",
    "# Finding the best number of clusters (k) for KMeans\n",
    "sse = []\n",
    "for i in range(1, 13):\n",
    "    KM = KMeans(n_clusters=i)\n",
    "    KM.fit(transform)\n",
    "    sse.append(KM.inertia_)\n",
    "plt.plot(range(1, 13), sse)\n",
    "plt.title(\"Number of Clusters vs SSE\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Sum of Squared Error(SSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As shows from above, the optimal value of K is 3\n",
    "km = KMeans(n_clusters=4, max_iter=1)\n",
    "km.fit(transform)\n",
    "predicted_clusters = km.predict(transform)\n",
    "centers = km.cluster_centers_\n",
    "\n",
    "# Draw the centriod\n",
    "markers = ['o', 'x', 's']\n",
    "colors = ['purple', 'b', 'yellow']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(transform[:, 0], transform[:, 1], c=predicted_clusters, s=7)\n",
    "for ind in range(len(markers)):\n",
    "    plt.scatter(centers[ind, 0], centers[ind, 1], c=colors[ind], s=100, marker=markers[ind])\n",
    "plt.title('Result of k-means Clustering', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "df_X['cluster'] = predicted_clusters\n",
    "cluster0 = df_X[df_X['cluster'] == 0]\n",
    "cluster1 = df_X[df_X['cluster'] == 1]\n",
    "cluster2 = df_X[df_X['cluster'] == 2]\n",
    "\n",
    "\n",
    "df_cluster0_description=cluster0.describe()\n",
    "df_cluster1_description=cluster1.describe()\n",
    "df_cluster2_description=cluster2.describe()\n",
    "\n",
    "# # # https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(df_cluster0_description)\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(df_cluster1_description)\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(df_cluster2_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Finding the differences within the features of each cluster. \n",
    "<a id='category_id'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of movies rented for each clusters based on the categories.\n",
    "df_category=pd.DataFrame()\n",
    "    \n",
    "#Obtains the category_id of the highest frequently rented movie within each cluster and then rename it.\n",
    "for cluster_no in range(3):\n",
    "    s_count_category_id = df_X[df_X['cluster'] == cluster_no].groupby(\"category_id\").count().rating\n",
    "    s_count_category_id.index=df_category_name.name\n",
    "    df_category[cluster_no]=s_count_category_id\n",
    "\n",
    "#Minor changes to output dataframe.\n",
    "df_category=df_category.T\n",
    "df_category = df_category.rename_axis([\"Cluster Name\"],axis=0).rename_axis([\"Categories\"],axis=1)\n",
    "s = df_category.style.background_gradient(cmap='coolwarm')\n",
    "print(\"**The number of movies rented for each clusters based on the categories.**\")\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_description_selector(measure,list_df_clusters):\n",
    "    df_clusters_description=pd.DataFrame(list_df_clusters[0].loc[measure]).T\n",
    "    for x in range(1,len(list_df_clusters)):\n",
    "        df_clusters_description=df_clusters_description.append(list_df_clusters[x].loc[measure])\n",
    "    df_clusters_description=df_clusters_description.drop(\"cluster\",axis=1)\n",
    "    df_clusters_description.index=[\"Cluster Number \"+ str(x) for x in range(3)]\n",
    "    return df_clusters_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster's Attributes's Means\n",
    "<a id='means'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the differences in the means of the attributes of each cluster.   \n",
    "list_df_cluster=[df_cluster0_description,df_cluster1_description,df_cluster2_description]\n",
    "print(\"**Means of various features of each cluster**\")\n",
    "dataframe_description_selector(\"mean\",list_df_cluster) \\\n",
    "                       .style.apply(highlight_column,column_name=\"length\") \\\n",
    "                       .apply(highlight_column,column_name=\"rating\",color=\"lightblue\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard Deviation of  Cluster's Attributes.\n",
    "<a id='std'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the differences in the standard deviation of the attributes of each cluster. \n",
    "list_df_cluster=[df_cluster0_description,df_cluster1_description,df_cluster2_description]\n",
    "print(\"**Standard Deviation of various features of each cluster**\")\n",
    "dataframe_description_selector(\"std\",list_df_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count & minimums/maximums of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**The count of films watched by customer in each cluster.**\")\n",
    "dataframe_description_selector(\"count\",list_df_cluster)[\"length\"].to_frame().rename({\"length\":\"Count\"},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum & maximum of the length attribute of each cluster\")\n",
    "df_min_max_clusters=dataframe_description_selector(\"min\",list_df_cluster).length\n",
    "df_min_max_clusters=df_min_max_clusters.to_frame()\n",
    "df_min_max_clusters[\"Max\"]=dataframe_description_selector(\"max\",list_df_cluster).length.values\n",
    "df_min_max_clusters=df_min_max_clusters.rename({\"length\":\"Min\"},axis=1)\n",
    "df_min_max_clusters[\"Range\"]=df_min_max_clusters.Max-df_min_max_clusters.Min\n",
    "df_min_max_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"> Your justifications & commentary here: </span>\n",
    "- We drop the feature \"Year\" because every vaule of it is 2006, which makes no contribution to the model.\n",
    "- The k value was decided based on using elbow method to find the optimal k clusters such that it has a minimised misrepresentation of cluster. The Point at which there is a significant change in Sum of squared error resulting in a notable, sharp dip in the plot, is the elbow point for the given data. Based on the elbow method's result, the appropriate k value (k = 3) is used for clustering.\n",
    "- The dataset is equally distributed so that the clustering is not obvious and not very meaningful.\n",
    "- We have tried multiple values but since the dataset is equally distributed, the number of iterations won't be very high. So we decided to use max interactions 30.\n",
    "- The number of data points in cluster 0 has a notable difference when compared to the other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do your resulting clusters represent?  Explain the distinguishing characteristics of each cluster. \n",
    "<span style= 'float: right;'><b>[2 marks]</b></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"> Your answer here: </span>\n",
    "Based on [the means](#means),it notable that the length of the films has some significant differences.Furthermore, the constant standard deviation [the standard deviation](#std) across all cluster ensures that the each cluster's length of films has some effect on the respective cluster.The cluster with the most count of datapoints, has the longest  average film length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Classification (30 Marks)\n",
    "\n",
    "The rental company has decided that they want to use simple machine learning to allocate price tags to their movies. The tags are as follows:\n",
    "\n",
    "\n",
    "|  **Classification**      |  **Requirements**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  Cheap                   |  Rental Rate is 0.99   |\n",
    "|  Moderate                |  Rental Rate is 2.99   |\n",
    "|  Expensive               |  Rental Rate is 4.99  |\n",
    "\n",
    "\n",
    "**Your task is to implement a `Classification Algorithm` (such as K-Nearest Neighbours) that can predict the `Price Label` of a movie**. You are required to perform the following tasks:\n",
    "\n",
    "1. Create useable dataset/s by manually determining the 'truth values' for existing data (where the rating is within the classification system defined above)\n",
    "2. Implement an algorithm that can predict the classification as per the above classifications.\n",
    "3. Perform independent testing of the model and provide statistical metrics outlining the performance of your model. Splitting the dataset into testing and training subsets will assist with this.\n",
    "\n",
    "You are welcome to use any features within the dataset, except the `Rental Rate` of the film. Various attributes relating to a movie in the tables `rating`, `movie`, `film_actor`, `actor` and `film_category` can be helpful while making the algorithm. If required, you can also look to make new **compound attributes** that may be helpful in increasing the accuracy of your model.\n",
    "You are expected to justify all aspects of your answer including the features used, the metrics provided and validation system employed. Provide commentary on the strengths and potential pitfalls of the model.\n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution. This means packages such as Keras, Tensorflow etc are not available for use.</span> \n",
    "<span style= 'float: right;'><b>[25 marks total: 18 marks model, 7 marks justification & commentary]</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading appropriate data\n",
    "df_classification_original = df_cleaned[[\"category_id\",\"film_id\", \"rental_rate\", \"rating\", \\\n",
    "                                         \"length\", \"rental_duration\", \"replacement_cost\",\"popularity_sum\", \"amount\"]]\n",
    "\n",
    "# Converting dtypes of columns accordingly \n",
    "df_classification_original.replacement_cost=df_classification_original.replacement_cost.astype(np.int64)\n",
    "df_classification_original.rental_rate=df_classification_original.rental_rate.astype(np.int64)\n",
    "\n",
    "\n",
    "# Converting categorical data(rating)\n",
    "le = LabelEncoder()\n",
    "df_classification_original.rating = le.fit_transform(df_classification_original.rating)\n",
    "\n",
    "\n",
    "\n",
    "df_classification_original_clone=df_classification_original\n",
    "df_classification_original_clone=df_classification_original_clone.dropna()\n",
    "df_classification_original_clone.amount=df_classification_original_clone.amount.astype(np.float64)\n",
    "\n",
    "# Replacing amounts with it's mean values & joining dataframes based on film_id\n",
    "df_amount_mean=df_classification_original_clone.groupby(\"film_id\").agg({\"amount\":\"mean\"})\n",
    "\n",
    "# # Getting other columns which are constant for each respective film_id \n",
    "df_classification_original = df_classification_original.drop([\"amount\"], axis=1).drop_duplicates()\n",
    "\n",
    "\n",
    "# # Joining the df_amount_means to the previous dataframe\n",
    "df_classification_original = df_classification_original.join(df_amount_mean, on=\"film_id\")\n",
    "df_classification_original = df_classification_original.reset_index().drop([\"film_id\"], axis=1)\n",
    "df_classification_original = df_classification_original.astype(np.int64)  # For KNN function data should be in integers\n",
    "df_classification_original=df_classification_original.drop([\"index\"],axis=1)\n",
    "df_classification_original\n",
    "\n",
    "# df_classification_original\n",
    "\n",
    "# # df_classification_original_clone.dropna()\n",
    "\n",
    "# #Making the currently rented film amounts as 0.\n",
    "\n",
    "# df_classification_original.amount=df_classification_original.amount.astype(np.float64)\n",
    "# print(df_classification_original.shape)\n",
    "\n",
    "\n",
    "# Replacing amounts with it's mean values & joining dataframes based on film_id\n",
    "# df_amount_mean=df_classification_original.groupby(\"film_id\").agg({\"amount\":\"mean\"})\n",
    "# print(df_amount_mean)\n",
    "\n",
    "\n",
    "# # Getting other columns which are constant for each respective film_id \n",
    "# df_classification_original = df_classification_original.drop([\"amount\"], axis=1).drop_duplicates()\n",
    "\n",
    "# # Joining the df_amount_means to the previous dataframe\n",
    "# df_classification_original = df_classification_original.join(df_amount_mean, on=\"film_id\")\n",
    "# df_classification_original = df_classification_original.reset_index().drop([\"film_id\"], axis=1)\n",
    "# df_classification_original = df_classification_original.astype(np.int64)  # For KNN function data should be in integers\n",
    "# df_classification_original=df_classification_original.drop([\"index\"],axis=1)\n",
    "\n",
    "# df_classification_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classification(df_classification,features):\n",
    "    '''Based on given features,a KNN classification will be done for all possible combinations.\n",
    "       @param df_classification: The input dataframe which contans the dependent variable and the other features\n",
    "       @param features: A list which contains features which should be recognised by the classification\n",
    "       @result df_scores: A dataframe which contains all model's Accuracy Scores '''\n",
    "\n",
    "    #Initialisations\n",
    "    kvalues=int(np.sqrt(df_classification.shape[0]))\n",
    "    # Making sure k values is odd.\n",
    "    kn = KNeighborsClassifier(n_neighbors=(kvalues if kvalues%2!=0 else kvalues+1))\n",
    "    all_combinations = feature_combinations(df_classification[features], \"rental_rate\")\n",
    "    df_scores = pd.DataFrame()\n",
    "\n",
    "    # Generating models for each respective combination.\n",
    "    for combination in all_combinations:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_classification[list(combination)], \\\n",
    "                                                        df_classification.rental_rate, test_size=0.33)\n",
    "        fit = kn.fit(x_train, y_train)\n",
    "        df_scores[str(list(combination))] = [np.mean(cross_val_score(kn, x_test, y_test, cv=10, scoring='accuracy'))]\n",
    "        \n",
    "        #On the basis current combination generate models for different types of reduced dimensionality.\n",
    "         # trying a combination in which dimensionality of the features are reduced using PCA.  \n",
    "        if (len(combination) > 1):\n",
    "            for i in range(len(combination) - 1):          \n",
    "                pca = PCA(n_components=i + 1)\n",
    "                data_x = pca.fit_transform(df_classification[list(combination)])\n",
    "                x_train, x_test, y_train, y_test = train_test_split(data_x, df_classification.rental_rate, test_size=0.33)\n",
    "                fit = kn.fit(x_train, y_train)\n",
    "                df_scores[\"PCA_\" + str(i + 1) + str(list(combination))] = [np.mean(cross_val_score(kn, data_x, df_classification.rental_rate, cv=10, scoring='accuracy'))]\n",
    "\n",
    "    #Making Dataframe more comprehensible .           \n",
    "    df_scores.index = [\"Scores\"]\n",
    "    df_scores = df_scores.T\n",
    "    df_scores = df_scores.sort_values(by=[\"Scores\"], ascending=False)\n",
    "\n",
    "    # # https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(df_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classification(df_classification_original,df_classification_original.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Variance Inflation Factor for the feature in df_classification.\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df_classification_original.values, i) for i in range(df_classification_original.shape[1])]\n",
    "vif[\"Features\"] = df_classification_original.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing amount as a feature & doing classification again.\n",
    "knn_classification(df_classification_original,df_classification_original.columns[~df_classification_original.columns.str.contains(\"amount\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Support Vector Machines\n",
    "svmo = svm.SVC(kernel='linear') \n",
    "features=df_classification_original.columns[~df_classification_original.columns.str.contains(\"amount\")]\n",
    "all_combinations=feature_combinations(df_classification_original[features],\"rental_rate\")\n",
    "df_scores=pd.DataFrame()\n",
    "\n",
    "for combination in all_combinations:\n",
    "    x_train,x_test,y_train,y_test=train_test_split(df_classification_original[list(combination)],df_classification_original.rental_rate,test_size=0.2)\n",
    "    svmo = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    svmo.fit(x_train, y_train)\n",
    "    y_pred = svmo.predict(x_test)\n",
    "    df_scores[str(list(combination))]=[metrics.accuracy_score(y_test, y_pred)]\n",
    "            \n",
    "df_scores.index=[\"Scores\"]\n",
    "df_scores=df_scores.T\n",
    "df_scores=df_scores.sort_values(by=[\"Scores\"],ascending=False)\n",
    "\n",
    "# # https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "    display(df_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"> Your justifications & commentary here: </span>\n",
    "\n",
    "- We explored that dataset and found out that data itselft has logic issues that could influence the model. Giving the same film, store, rental duration and rental rate, the amount should be the same but it is not. \n",
    "\n",
    "\n",
    "- When building the model, we first created the K-Nearest Neighbours algorithm with the category_id\",\"film_id\", \"rental_rate\", \"rating\", \"length\", \"rental_duration\", \"replacement_cost\", \"amount\" and got the perfect score of about 0.97. The results show that we are overfitting the data and we suspected that some independent variables have multicollinearity to one another. So, we tried to find out by using the variance inflation factor method (VIF) which indicates the variables are not completely independent of each other if the result is over 10. We also find out that the VIF of feature 'amount' is over 27 which shows very serious multicollinearity. So, we re-ran K-Nearest Neighbours algorithms without the feature 'amount' and it got a score of 0.4 which is our final prediction. We also used the Cross-validation method to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Would you be able to get a better result, if you had used Clustering as a pre-processing step before Classfication? Justify your answer. \n",
    "<span style= 'float: right;'><b>[5 marks]</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\"> Your answer here: </span>\n",
    "\n",
    "- For this kind of large dataset,it is better to have KMeans clustering as a pre_processing step before classification.By using clustering as the pre-processing, it minimizes the constraint that the original KNN have i.e high level of computation complexity due to the large training sample data. That means if training data is too large, the KNN algorithmn takes more time to calculate.However,when we use clustering as a pre-process, it not only prevent us from taking time but also help us improve the data accuracy, and minimize processing time. The only constraint of using this combination algorithmns is only on the small data so that in the large dataset like ours have no issues to implement. \n",
    "\n",
    "- We can use this combination by grouping training data of each classification to the  K-means algorithm, and take all the centroids of clusters as the new training data. All training data that we got from the clusters will be used in classification with KNN algorithm to calculate the accuracy.\n",
    "\n",
    "\n",
    "Referenced from\n",
    "Buana, P.W., Jannet, S.D.R.M. and Putra, I.K.G.D., 2012. Combination of k-nearest neighbor and k-means based on term re-weighting for classify Indonesian news. International Journal of Computer Applications, 50(11), pp.37-42.\n",
    "\n",
    "\n",
    "To be removed Later\n",
    " Second, grouping\n",
    "all the training samples of each category of K-means algorithm,\n",
    "and take all the cluster centers as the new training sample\n",
    "\n",
    " Third,\n",
    "the modified training samples are used for classification with\n",
    "KNN algorithm. Finally, calculate the accuracy of the evaluation\n",
    "using precision, recall and f-measure. \n",
    "\n",
    "** The traditional KNN text classification has three limitations.\n",
    "First, high calculation complexity to find out the k nearest\n",
    "neighbor samples, all the similarities between the training\n",
    "samples must be calculated. With less training samples,\n",
    "calculation time is not significant, but if the training set contains\n",
    "a huge number of samples, the KNN classifier needs more time\n",
    "to calculate the similarities.So, it can  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D - Decision Trees (10 Marks)\n",
    "\n",
    "The following is a small synthetic data set about the weather conditions.  We are\n",
    "going to try and use decision trees to predict whether it will rain or not on the given day.\n",
    "\n",
    "\n",
    "|Temperature| Cloudy| UV Index| Humidity| Rain\n",
    "|---:|--:|--:|--:|--:|\n",
    "|25|No| Low| Low| No \n",
    "|29|No| Low| High| No\n",
    "|26|No| Low| Medium| No\n",
    "|26|No| Medium| Medium| No\n",
    "|27|No| Medium| High| No\n",
    "|28|No| High | High| No\n",
    "|25|No| High |Low| No\n",
    "|29|Yes| Low |Low| Yes\n",
    "|28|No| Medium| High| Yes\n",
    "|28|Yes| Medium| High| Yes\n",
    "|26|No| Low |Low| Yes\n",
    "|27|Yes| Low |High| Yes\n",
    "\n",
    "**Note:**\n",
    "* You can treat temperature as a continuous variable and split it on a range of values (to convert it to a categorical variable, for example).\n",
    "* Attribute selection in the tree uses information gain.\n",
    "* You can use LaTeX and/or markdown to format your equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rain = pd.read_csv(\"rain.csv\")\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "replacement_items = {\"No\": 0, \"Yes\": 1, \"Low\": 1, \"Medium\": 2, \"High\": 3}\n",
    "df_rain = df_rain.replace(replacement_items)\n",
    "df_rain_X = df_rain.drop(labels=['Rain'], axis=1)\n",
    "df_rain_y = df_rain['Rain']\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", presort=True)\n",
    "dt.fit(df_rain_X, df_rain_y)\n",
    "dotf = export_graphviz(dt, out_file='decision_tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the initial entropy of Cloudy?\n",
    "\n",
    "<span style= 'float: right;'><b>[3 marks]</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "#### By using the formula of entrophy which is $H(X)=\\sum_{x\\epsilon X} p(x) log_2p(x)$ we get $-\\frac{3}{12}(log_2 \\frac{3}{12}) -\\frac{9}{12}(log_2 \\frac{9}{12})$, The result of  initial entropy of Cloudy will be approximately 0.8112\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Which attribute would the decision-tree building algorithm choose at the root of the tree?\n",
    "\n",
    "<span style= 'float: right;'><b>[3 marks]</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "#### Cloudy would be the best attribute for choosing the decision tree building because it has more information gain than the other attributes.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate and specify the information gain of the attribute you chose to split on in the previous question\n",
    "\n",
    "<span style= 'float: right;'><b>[4 marks]</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "#### YOUR ANSWER HERE\n",
    "\n",
    "\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
